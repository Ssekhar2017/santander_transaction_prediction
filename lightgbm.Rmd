---
title: "Lightgbm"
author: "Amin Yakubu"
date: "2/27/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(data.table)
library(Matrix)
library(dplyr)
library(MLmetrics)
library(lightgbm)
library(tidyverse)
```

```{r}
sntdr_train = read_csv("./data/train.csv")
sntdr_test = read_csv("./data/test.csv")
```

Creating a matrix and partitioning my data into train and testing

```{r}
X = sntdr_train %>% select(-ID_code, -target) %>% data.matrix()
y = sntdr_train$target

set.seed(25)
train = sample(1:nrow(X), nrow(X)/2)
test = (-train)

X.train = X[train,]
y.train = y[train]

X.test = X[test,]
y.test = y[test]
```

```{r}
dtrain <- lgb.Dataset(data = X.train,
                      label = y.train,
                      free_raw_data = FALSE)

dtest <- lgb.Dataset.create.valid(dtrain, data = X.test, label = y.test)
```

Using validation set 

```{r}
valids <- list(train = dtrain, test = dtest)
```

Searching for the best parameters 

```{r}
grid_search <- expand.grid(max_depth = c(10,20,40,80),
                           min_data_in_leaf = c(1,2,4),
                           min_sum_hessian_in_leaf = c(0.05,0.1,0.2),
                           feature_fraction = c(0.8,0.9,0.95),
                           bagging_fraction = c(0.4,0.6),
                           bagging_freq = c(2,4),
                           lambda_l1 = c(0.2,0.4),
                           lambda_l2 = c(0.2,0.4),
                           min_gain_to_split = c(0.2,0.4))

model <- list()
perf <- numeric(nrow(grid_search))

for (i in 1:nrow(grid_search)) {
  model[[i]] <- lgb.train(list(objective = "binary",
                          metric = "auc",
                          max_depth = grid_search[i, "max_depth"],
                          min_data_in_leaf = grid_search[i,"min_data_in_leaf"],
                          min_sum_hessian_in_leaf = grid_search[i, "min_sum_hessian_in_leaf"],
                          feature_fraction =  grid_search[i, "feature_fraction"],
                          bagging_fraction =  grid_search[i, "bagging_fraction"],
                          bagging_freq =  grid_search[i, "bagging_freq"],
                          lambda_l1 =  grid_search[i, "lambda_l1"],
                          lambda_l2 =  grid_search[i, "lambda_l2"],
                          min_gain_to_split =  grid_search[i, "min_gain_to_split"]),
                          dtrain,
                          2,
                          valids,
                          #min_data = 1,
                          num_leaves = 100,
                          learning_rate = 0.1,
                          early_stopping_rounds = 20)
  
  perf[i] <- min(rbindlist(model[[i]]$record_evals$test$l2))
}
# grid_search
cat("Model ", which.min(perf), " is lowest loss: ", min(perf), sep = "","\n")
print(grid_search[which.min(perf), ])

```
















Setting parameters 

```{r}
lgb.grid = list(objective = "binary",
                num_leaves = 10,
                metric = "auc",
                min_sum_hessian_in_leaf = 0.00245,
                feature_fraction = 0.05,
                bagging_fraction = 1.0,
                bagging_freq = 5,
                max_bin = 119,
                lambda_l1 = 4.972,
                lambda_l2 = 2.276,
                min_data_in_bin = 100,
                min_gain_to_split = 0.65,
                min_data_in_leaf = 11,
                is_unbalance = TRUE,
                boost_from_average = FALSE,
                boosting_type = 'dart',
                max_depth = 14,
                save_binary = TRUE,
                learning_rate = 0.009)
```

```{r}
print("Train lightgbm using lgb.train with valids")
set.seed(28)
bst <- lgb.train(params = lgb.grid, 
                 data = dtrain,
                 nrounds = 5000,
                 valids = valids)

best.iter = bst$best_iter
best.iter
```


```{r}
lgb.model.cv = lgb.cv(params = lgb.grid, data = dtrain, 
                    num_threads = 2, nrounds = 10, early_stopping_rounds = 50,
                   eval_freq = 20, eval = 'auc', verbose = 1,
                    stratified = TRUE)

cv.best.iter = lgb.model.cv$best_iter
cv.best.iter

```

# Prediction

```{r}
pred <- predict(bst, X.test, num_iteration = best.iter)

err <- mean(as.numeric(pred > 0.5) != y.test)
print(paste("test-error=", err))
```

Submission

```{r}
test_matrix = sntdr_test %>% select(-ID_code) %>% data.matrix()

snt_pred <- predict(bst, test_matrix, num_iteration = best.iter)
submission = tibble(ID_code = sntdr_test$ID_code,
                    target = snt_pred)

write_csv(submission, path = "./data/final_submission-R.csv")
```

